# -*- coding: utf-8 -*-
"""hw5_template (2).ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/17yfGiJ9ND1LERFWAOfmP7dGUZaCoNIYk
"""

from google.colab import drive
import os
drive.mount('/content/drive')

os.chdir('/content/drive/MyDrive/Colab Notebooks/HW5')

import numpy as np
import os
import math
import csv
import matplotlib.pyplot as plt
# Import the packages you need here

data = np.load('data.npz')
label = np.load('label.npz')

X_train = data['X_train']
X_val = data['X_val']
X_test = data['X_test']

Y_train = label['Y_train']
Y_val = label['Y_val']

#print(Y_train)

X_train.shape, X_val.shape, X_test.shape

Y_train.shape, Y_val.shape

#print(X_train)

# Build your model here:
import tensorflow as tf
import keras
from keras.models import Sequential
from keras import Input
from keras.layers import Conv1D, MaxPooling1D
from keras.layers import Embedding, LSTM, Dense,TimeDistributed, RepeatVector, Bidirectional, Dropout, Activation
from keras.optimizers import SGD
from keras.layers import BatchNormalization


model = Sequential()

model.add(BatchNormalization())
model.add(LSTM(256, activation='relu', input_shape=(22,200)))
model.add(Dense(6))
model.add(Activation('softmax'))
model.compile(
    loss=keras.losses.SparseCategoricalCrossentropy(from_logits=True),
    optimizer='rmsprop',
    metrics=["accuracy"],
)
#model.compile(loss='categorical_crossentropy',
#              optimizer='rmsprop',
#              metrics=['accuracy'])
model.build(X_train.shape)
model.summary()
history = model.fit(X_train, Y_train, epochs=100, validation_data=(X_val, Y_val))

plt.plot(history.history['loss'])
plt.plot(history.history['val_loss'])
plt.title('LSTM Model loss')
plt.ylabel('loss')
plt.xlabel('epoch')
plt.legend(['train', 'val'], loc='upper left')
plt.show()

pred = model.predict(X_test)
#print(pred)
#output = output.reshape(-1,1)

#print(output.reshape(-1))
#print(pred.shape)
output = (np.argmax(pred, axis=1)).reshape(-1,1)
#print(output)
print(output.flatten())
#print(output.shape)
assert(output.shape == (190, 1))
np.savetxt('lstm_output.csv', output, delimiter=",")

# Build your model here:
from EEGModels import EEGNet

model1 = EEGNet(nb_classes = 6, Chans = X_train.shape[1], Samples = X_train.shape[2])

model1.compile(loss=keras.losses.SparseCategoricalCrossentropy(from_logits=True), optimizer='adam',
              metrics = ['accuracy'])

model1.summary()

history1 = model1.fit(X_train, Y_train, epochs=100, validation_data=(X_val, Y_val))

plt.plot(history1.history['loss'])
plt.plot(history1.history['val_loss'])
plt.title('EEGNet Model loss')
plt.ylabel('loss')
plt.xlabel('epoch')
plt.legend(['train', 'val'], loc='upper left')
plt.show()

pred1 = model1.predict(X_test)
#print(pred)
#print(pred.shape)

output = (np.argmax(pred1, axis=1)).reshape(-1,1)
#print(output.shape)
print(output.flatten())
assert(output.shape == (190, 1))
np.savetxt('eegnet_output.csv', output, delimiter=",")

# Build your model here:
from EEGModels import EEGNet

model2 = EEGNet(nb_classes = 6, Chans = X_train.shape[1], Samples = X_train.shape[2])

model2.compile(loss=keras.losses.SparseCategoricalCrossentropy(from_logits=True), optimizer='adam',
              metrics = ['accuracy'])

model2.summary()

history2 = model2.fit(X_train, Y_train, epochs=100, validation_data=(X_val, Y_val))

plt.plot(history2.history['loss'])
plt.plot(history2.history['val_loss'])
plt.title('EEGNet Model loss')
plt.ylabel('loss')
plt.xlabel('epoch')
plt.legend(['train', 'val'], loc='upper left')
plt.show()

pred2 = model2.predict(X_test)

output = (np.argmax(pred2, axis=1)).reshape(-1,1)
print(output.flatten())
assert(output.shape == (190, 1))
np.savetxt('competition_output.csv', output, delimiter=",")