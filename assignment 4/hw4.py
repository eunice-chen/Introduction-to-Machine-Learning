# -*- coding: utf-8 -*-
"""hw4 (1).ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1pYk80GK7errgrlpRnQQQIeU6xHcqsh0g
"""

from google.colab import drive
drive.mount('/content/drive')

import sys
sys.path.append("/content/drive/MyDrive/Colab Notebooks") ## the path of the directory where you place dense.py, activation.py ....

import os
import math
import time
import numpy as np
from cv2 import imread, IMREAD_GRAYSCALE # IMREAD_GRAYSCALE allow you to load the image as gray scale image
from pandas import read_csv
import matplotlib.pyplot as plt

###### import your HW3 code (Don't change this part) ######
from Dense import Dense
from Activation import Activation
from Loss import compute_BCE_cost
from Predict import predict
##################################

output = {}
seed = 1
np.random.seed(seed)
num_parameters = 0

def zero_pad(X, pad):
    """
    Pad with zeros all images of the dataset X. The padding is applied to the height and width of an image.

    Argument:
    X -- python numpy array of shape (m, n_H, n_W, n_C), where m represent the number of examples.
    pad -- integer, amount of padding around each image on vertical and horizontal dimensions

    Returns:
    X_pad -- padded image of shape (m, n_H + 2*pad, n_W + 2*pad, n_C)
    """

    # GRADED FUNCTION: zero_padding
    ### START CODE HERE ### (≈ 1 line)
    X_pad = np.pad(X, ((0, 0), (pad, pad), (pad, pad), (0, 0)), mode = 'constant', constant_values = (0,0))
    ### END CODE HERE ###

    return X_pad

class Conv():
    def __init__(self, filter_size=2, input_channel=3, output_channel=8, pad=1, stride=1, seed=1):

        self.filter_size= filter_size
        self.input_channel=input_channel
        self.output_channel=output_channel
        self.seed = seed

        self.parameters = {"pad": pad, "stride": stride}
        self.initialize_parameters()


        self.name="conv"

    def initialize_parameters(self):
        """
        Argument:
        self.filter_size -- size of the filter
        self.input_channel -- size of the input channel
        self.output_channel -- size of the output channel
        self.parameters -- python dictionary containing your parameters:
                           W -- weight matrix of shape (filter_size, filter_size, input channel size, output channel size)
                           b -- bias vector of shape (1, 1, 1, output channel size)
                           pad -- amount of padding around each image on vertical and horizontal dimensions
                           stride -- represent the amount of movement that a filter move in one step
        """
        np.random.seed(seed)

        # GRADED FUNCTION: conv_initialization
        ### START CODE HERE ### (≈ 8 lines)
        #filter_size = self.filter_size*self.filter_size
        #input_channel = self.input_channel
        #output_channel = self.output_channel
        #pad = self.parameters["pad"]
        #stride = self.parameters["stride"]
        shape = (self.filter_size, self.filter_size, self.input_channel, self.output_channel)
        W = np.zeros(shape)
        for i in range(self.filter_size):
          for j in range(self.filter_size):
            for k in range(self.input_channel):
              for l in range(self.output_channel):
                W[i][j][k][l] = np.array(np.random.uniform((-1)*(np.sqrt(6/(self.input_channel+ self.output_channel))),(np.sqrt(6/(self.input_channel + self.output_channel)))))
        b = np.zeros((1, 1, 1, self.output_channel))*(np.sqrt(6/(self.input_channel + self.output_channel)))
        ### END CODE HERE ###

        assert(W.shape == (self.filter_size,self.filter_size,self.input_channel,self.output_channel))
        assert(b.shape == (1,1,1,self.output_channel))

        self.parameters['W'] = W
        self.parameters['b'] = b



    def conv_single_step(self, a_slice_prev, W, b):
        """
        Apply a filter W on a_slice_prev.

        Arguments:
        a_slice_prev -- slice of input data of shape (filter_size, filter_size, n_C_prev)
        W -- Weight parameters contained in a window - matrix of shape (filter_size, filter_size, n_C_prev)
        b -- Bias parameters contained in a window - matrix of shape (1, 1, 1)

        Returns:
        Z -- a scalar value, result of convolving the sliding window (W, b) on a slice x of the input data
        """

        # GRADED FUNCTION: conv_single_step
        ### START CODE HERE ### (≈ 3 lines)
        # Element-wise product between a_slice and W.
        #s = np.multiply(a_slice_prev, W.reshape((-1, 1)))
        #s = a_slice_prev * W[:, np.newaxis]
        #s = np.multiply(W, a_slice_prev[:,None])
        #s = a_slice_prev * W
        #s = np.matmul(a_slice_prev,diag(W))
        #print("in single step", a_slice_prev.shape)
        #print(W)
        s = np.multiply(a_slice_prev, W)

        # Sum over all entries of the volume s.
        Z = np.sum(s)
        # Add bias b to Z. Cast b to a float() so that Z results in a scalar value.
        Z = Z + float(b)
        ### END CODE HERE ###

        return Z

    def forward(self, A_prev):
        """
        Implements the forward propagation for a convolution layer

        Arguments:
        A_prev -- output activations of the previous layer, numpy array of shape (m, n_H_prev, n_W_prev, n_C_prev)

        Returns:
        Z -- conv output, numpy array of shape (m, n_H, n_W, n_C)
        """

        # GRADED FUNCTION: conv_forward
        ### START CODE HERE ###
        global num_parameters
        # Retrieve dimensions from A_prev's shape (≈1 line)
        (m, n_H_prev, n_W_prev, n_C_prev) = A_prev.shape

        # Retrieve dimensions from W's shape (≈1 line)
        (f, f, n_C_prev, n_C) = self.parameters["W"].shape

        num_parameters += (f * f * n_C_prev + 1) *  n_C
        #print(num_parameters)
        # Compute the dimensions of the convolution output volume using the formula given below.(≈2 lines)
        n_H = int(math.floor((n_H_prev-f+2*self.parameters["pad"])/self.parameters["stride"])) + 1
        n_W = int(math.floor((n_W_prev-f+2*self.parameters["pad"])/self.parameters["stride"])) + 1

        # Initialize the output volume Z with zeros. (≈1 line)
        Z = np.zeros((m, n_H, n_W, n_C))


        # if pad!=0, create A_prev_pad by padding A_prev with the parameter "pad". (≈1 line)
        A_prev_pad = zero_pad(A_prev, self.parameters["pad"])

        for i in range(m):                               # loop over the batch of training examples
            a_prev_pad = A_prev_pad[i,:,:,:]               # Select ith training example's padded activation
            for h in range(n_H):                           # loop over vertical axis of the output volume
                for w in range(n_W):                       # loop over horizontal axis of the output volume
                    for c in range(n_C):                   # loop over channels (= #filter) of the output volume

                        # Find the corners of the current "slice" (≈4 lines)
                        vert_start = h * self.parameters["stride"]
                        vert_end = h * self.parameters["stride"] + f
                        horiz_start = w * self.parameters["stride"]
                        horiz_end = w * self.parameters["stride"] + f


                        # Use the corners to define the slice of a_prev_pad. (≈1 line)
                        a_slice_prev = a_prev_pad[vert_start:vert_end, horiz_start:horiz_end,:]

                        # Convolve the (3D) slice with the correct filter W and bias b, to get back one output neuron. (≈1 line)
                        Z[i, h, w, c] = self.conv_single_step(a_slice_prev, self.parameters["W"][:,:,:,c], self.parameters["b"][:,:,:,c])
                        #print(self.parameters["W"][:,:,:,c])
        ### END CODE HERE ###

        # Making sure your output shape is correct
        assert(Z.shape == (m, n_H, n_W, n_C))

        # Save information in "cache" for the backward pass
        self.cache = A_prev

        return Z

    def backward(self, dZ):
        """
        Implement the backward propagation for a convolution layer

        Arguments:
        dZ -- gradient of the cost with respect to the output of the conv layer (Z), numpy array of shape (m, n_H, n_W, n_C)

        Returns:
        dA_prev -- gradient of the cost with respect to the input of the conv layer (A_prev),
                   numpy array of shape (m, n_H_prev, n_W_prev, n_C_prev)
        """


        A_prev = self.cache

        # Retrieve dimensions from A_prev's shape
        (m, n_H_prev, n_W_prev, n_C_prev) = A_prev.shape

        # Retrieve dimensions from W's shape
        (f, f, n_C_prev, n_C) = self.parameters["W"].shape


        # Retrieve dimensions from dZ's shape
        (m, n_H, n_W, n_C) = dZ.shape

        # Initialize dA_prev, dW, db with the correct shapes
        dA_prev = np.zeros((m, n_H_prev, n_W_prev, n_C_prev))
        dW = np.zeros((f, f, n_C_prev, n_C))
        db = np.zeros((1, 1, 1, n_C))

        # Pad A_prev and dA_prev
        A_prev_pad = zero_pad(A_prev, self.parameters["pad"])
        dA_prev_pad = zero_pad(dA_prev, self.parameters["pad"])

        for i in range(m):                       # loop over the training examples

            # select ith training example from A_prev_pad and dA_prev_pad
            a_prev_pad = A_prev_pad[i]
            da_prev_pad = dA_prev_pad[i]

            for h in range(n_H):                   # loop over vertical axis of the output volume
                for w in range(n_W):               # loop over horizontal axis of the output volume
                    for c in range(n_C):           # loop over the channels of the output volume

                        # Find the corners of the current "slice"
                        vert_start = h * self.parameters["stride"]
                        vert_end = vert_start + f
                        horiz_start = w * self.parameters["stride"]
                        horiz_end = horiz_start + f

                        # Use the corners to define the slice from a_prev_pad
                        a_slice = a_prev_pad[vert_start:vert_end, horiz_start:horiz_end, :]

                        # Update gradients for the window and the filter's parameters
                        da_prev_pad[vert_start:vert_end, horiz_start:horiz_end, :] += self.parameters["W"][:,:,:,c] * dZ[i, h, w, c]
                        dW[:,:,:,c] += a_slice * dZ[i, h, w, c]
                        db[:,:,:,c] += dZ[i, h, w, c]

            # Set the ith training example's dA_prev to the unpaded da_prev_pad
            dA_prev[i, :, :, :] = da_prev_pad[self.parameters["pad"]:da_prev_pad.shape[0]-self.parameters["pad"],
                                              self.parameters["pad"]:da_prev_pad.shape[1]-self.parameters["pad"], :]

        assert(dA_prev.shape == (m, n_H_prev, n_W_prev, n_C_prev))

        self.dW = dW
        self.db = db

        return dA_prev

    def update(self, learning_rate):
        """
        Update parameters using gradient descent

        Arguments:
        learning rate -- step size
        """

        # GRADED FUNCTION: conv_update
        ### START CODE HERE ### (≈ 2 lines of code)
        self.parameters["W"] = self.parameters["W"] - learning_rate * self.dW
        self.parameters["b"] = self.parameters["b"] - learning_rate * self.db
        ### END CODE HERE ###

np.random.seed(seed)
conv = Conv(filter_size=2, input_channel=3, output_channel=8, pad=2, stride=2)
print("W[0][0][0] = ",  conv.parameters["W"][0][0][0])
print("b = ", conv.parameters["b"])

np.random.seed(seed)
conv = Conv(filter_size=2, input_channel=3, output_channel=16, pad=2, stride=2)
output["conv_initialization"] = conv.parameters["W"][0][0][0]

np.random.seed(seed)
x = np.random.randn(4, 3, 3, 2)
x_pad = zero_pad(x, 2)
print ("x.shape =\n", x.shape)
print ("x_pad.shape =\n", x_pad.shape)
print ("x[0,2,:,0] =\n", x[0,2,:,0])
print ("x_pad[0,2,:,0] =\n", x_pad[0,2,:,0])

fig, axarr = plt.subplots(1, 2)
axarr[0].set_title('x')
axarr[0].imshow(x[0,:,:,0])
axarr[1].set_title('x_pad')
axarr[1].imshow(x_pad[0,:,:,0])

np.random.seed(seed)
x = np.random.randn(4, 2, 2, 2)
x_pad = zero_pad(x, 1)
output["zero_padding"] = x_pad[0,1,:,0]

np.random.seed(seed)
a_slice_prev = np.random.randn(4, 4, 3)
W = np.random.randn(4, 4, 3)
b = np.random.randn(1, 1, 1)

conv = Conv(filter_size=2, input_channel=3, output_channel=8, pad=2, stride=2)
Z = conv.conv_single_step(a_slice_prev, W, b)
print("Z =", Z)

np.random.seed(seed)
a_slice_prev = np.random.randn(3, 3, 3)
W = np.random.randn(3, 3, 3)
b = np.random.randn(1, 1, 1)
conv = Conv()
Z = conv.conv_single_step(a_slice_prev, W, b)
output["conv_single_step"] = Z

np.random.seed(seed)
A_prev = np.random.randn(10,4,4,3)
conv=Conv(filter_size=2, input_channel=3, output_channel=8, pad=2, stride=2)
Z = conv.forward(A_prev)

print("Z's mean =", np.mean(Z))
print("Z[3,2,1] =", Z[3,2,1])
print("cache_conv[1][2][3] =", conv.cache[1][2][3])


np.random.seed(seed)
A_prev = np.random.randn(10,3,3,3)
conv=Conv(filter_size=3, input_channel=3, output_channel=16, pad=1, stride=1)
Z = conv.forward(A_prev)

output["conv_forward_1"] = np.mean(Z)
output["conv_forward_2"] = Z[3,2,1]
output["conv_forward_3"] = conv.cache[1][2][2]

conv=Conv(filter_size=2, input_channel=3, output_channel=8, pad=2, stride=2)
np.random.seed(seed)
conv.dW = np.random.randn(2, 2, 3, 8)
conv.db = np.random.randn(1, 1, 1, 8)
conv.update(1.0)
print("W[0][0][0] = ", conv.parameters["W"][0][0][0])
print("b = ", conv.parameters["b"])

conv=Conv(filter_size=3, input_channel=3, output_channel=8, pad=1, stride=2)
np.random.seed(seed)
conv.dW = np.random.randn(3, 3, 3, 8)
conv.db = np.random.randn(1, 1, 1, 8)
conv.update(0.1)
output["conv_update_1"] = conv.parameters["W"][0][0][0]
output["conv_update_2"] = conv.parameters["b"]

class MaxPool():
    def __init__(self, filter_size=2, stride=2):
        """
        Argument:
        self.parameters -- python dictionary containing your parameters:
                           f -- size of a filter
                           stride -- the amount of movement that a filter move in one step
        """

        self.parameters = {"f": filter_size, "stride": stride}
        self.name="maxpool"


    def forward(self, A_prev):
        """
        Implements the forward pass of the max pooling layer

        Arguments:
        A_prev -- Input data, numpy array of shape (m, n_H_prev, n_W_prev, n_C_prev)

        Returns:
        A -- output of the pool layer, a numpy array of shape (m, n_H, n_W, n_C)
        """

        # GRADED FUNCTION: maxpool_forward
        ### START CODE HERE ###
        # Retrieve dimensions from the input shape. (≈1 line)
        (m, n_H_prev, n_W_prev, n_C_prev) = A_prev.shape


        # Define the dimensions of the output. (≈3 lines)
        #n_H = int((n_H_prev - self.parameters["f"]) / self.parameters["stride"]) + 1
        #n_W = int((n_W_prev - self.parameters["f"]) / self.parameters["stride"]) + 1
        n_H = math.floor((n_H_prev - self.parameters["f"]) / self.parameters["stride"]) + 1
        n_W = math.floor((n_W_prev - self.parameters["f"]) / self.parameters["stride"]) + 1
        n_C = n_C_prev

        # Initialize output matrix A with zeros. (≈1 line)
        A = np.zeros((m, n_H, n_W, n_C))

        for i in range(m):                         # loop over the training examples
            for h in range(n_H):                     # loop on the vertical axis of the output volume
                for w in range(n_W):                 # loop on the horizontal axis of the output volume
                    for c in range (n_C):            # loop over the channels of the output volume

                        # Find the corners of the current "slice". (≈4 lines)
                        vert_start = h * self.parameters["stride"]
                        vert_end = h * self.parameters["stride"] + self.parameters["f"]
                        horiz_start = w * self.parameters["stride"]
                        horiz_end = w * self.parameters["stride"] + self.parameters["f"]

                        # Use the corners to define the current slice on the ith training example of A_prev, channel c. (≈1 line)
                        a_prev_slice = A_prev[i, vert_start:vert_end, horiz_start:horiz_end, c]

                        # Compute the max pooling operation on a_prev_slice. (≈1 line)
                        A[i, h, w, c] = np.max(a_prev_slice)

        ### END CODE HERE ###

        # Store the input in "cache" for backward pass
        self.cache = A_prev

        # Making sure your output shape is correct
        assert(A.shape == (m, n_H, n_W, n_C))

        return A

    def create_mask_from_window(self, x):
        """
        Creates a mask from an input x to identify the max entry of x.

        Arguments:
        x -- Array of shape (filter_size, filter_size)

        Returns:
        mask -- Array of the same shape as filter, contains a True at the position corresponding to the max entry of x.
        """

        mask = x == np.max(x)

        return mask

    def backward(self, dA):
        """
        Implements the backward pass of the max pooling layer

        Arguments:
        dA -- gradient of cost with respect to the output of the pooling layer, same shape as A

        Returns:
        dA_prev -- gradient of cost with respect to the input of the pooling layer, same shape as A_prev
        """

        # Retrieve information from cache
        A_prev = self.cache

        # Retrieve dimensions from A_prev's shape and dA's shape
        m, n_H_prev, n_W_prev, n_C_prev = A_prev.shape
        m, n_H, n_W, n_C = dA.shape

        # Initialize dA_prev with zeros
        dA_prev = np.zeros(A_prev.shape)

        for i in range(m):  # loop over the training examples
            # select training example from A_prev
            a_prev = A_prev[i]
            for h in range(n_H):   # loop on the vertical axis
                for w in range(n_W):  # loop on the horizontal axis
                    for c in range(n_C): # loop over the channels

                        # Find the corners of the current "slice"
                        vert_start = h * self.parameters["stride"]
                        vert_end = vert_start + self.parameters["f"]
                        horiz_start = w * self.parameters["stride"]
                        horiz_end = horiz_start + self.parameters["f"]

                        #Use the corners and "c" to define the current slice from a_prev
                        a_prev_slice = a_prev[vert_start:vert_end, horiz_start:horiz_end, c]
                        # Create the mask from a_prev_slice
                        mask = self.create_mask_from_window(a_prev_slice)
                        # Set dA_prev to be dA_prev + (the mask multiplied by the correct entry of dA)
                        dA_prev[i, vert_start:vert_end, horiz_start:horiz_end, c] += np.multiply(mask, dA[i, h, w, c])



        # Make sure your output shape is correct
        assert(dA_prev.shape == A_prev.shape)

        return dA_prev

np.random.seed(seed)
A_prev = np.random.randn(2, 4, 4, 3)
maxpool=MaxPool(filter_size=3, stride=2)
A = maxpool.forward(A_prev)
print("A =", A)

A_prev = np.random.randn(2, 5, 5, 3)
maxpool=MaxPool(filter_size=2, stride=1)
A = maxpool.forward(A_prev)
output["maxpool_forward"] = A

class Flatten():
    def __init__(self):
        self.name="flatten"

    def forward(self, A_prev):
        """
        Implements the forward pass of the flatten layer

        Arguments:
        A_prev -- Input data, numpy array of shape (m, n_H_prev, n_W_prev, n_C_prev)

        Returns:
        A -- output of the flatten layer, a 1-dimensional array
        """

        # Save information in "cache" for the backward pass
        self.cache = A_prev[0].shape

        # GRADED FUNCTION: flatten_forward
        ### START CODE HERE ### (≈1 line)
        #A =  np.reshape(A_prev, (A_prev.shape[0], int(A_prev.size / A_prev.shape[0])))
        A = np.ravel(A_prev).reshape(A_prev.shape[0], -1)
        #print(A.shape)
        #A = A_prev.reshape(A_prev.shape[0],np.prod(A_prev.shape[1:]))
        #print("in flatten forward's A = ", A.shape)
        #print("A in flatten forward: ", A.shape)
        #A = A_prev.reshape(A_prev.shape[0], -1)
        #print(A_prev.shape)
        #print(A_prev[1].shape)
        ### END CODE HERE ###
        return A

    def backward(self, dA):
        """
        Implements the backward pass of the flatten layer

        Arguments:
        dA -- Input data, a 1-dimensional array

        Returns:
        dA_prev -- An array with its original shape (the output shape of its' previous layer).
        """
        # GRADED FUNCTION: flatten_backward
        ### START CODE HERE ### (≈1 line)
        #print(self.cache)
        #print("0", self.cache[0])
        #print("1", self.cache[1])
        #print("2", self.cache[2])
        #print(dA)
        #dA_prev = np.reshape(dA, (-1, self.cache[0],self.cache[1],self.cache[2]))
        #dA_prev = dA.reshape(*self.cache)
        dA_prev = np.reshape(dA, (-1, *self.cache))
        #print(dA_prev.shape)
        ### END CODE HERE ###
        return dA_prev

np.random.seed(seed)
A_prev = np.random.randn(2,2,2,2)
flatten = Flatten()
A = flatten.forward(A_prev)
print("A.shape =", A.shape)
print("A[0] =", A[0])


np.random.seed(seed)
A_prev = np.random.randn(2,3,3,2)
flatten = Flatten()
A = flatten.forward(A_prev)
output["flatten_forward"] = A[0]

np.random.seed(seed)
A_prev = np.random.randn(2,2,2,2)
flatten = Flatten()
A = flatten.forward(A_prev)
B = flatten.backward(A)
print("B.shape =", B.shape)
print("B[0] =", B[0])

# B and A_prev should be same
assert((B==A_prev).all())

np.random.seed(seed)
A_prev = np.random.randn(4,3,3,3)
flatten = Flatten()
A = flatten.forward(A_prev)
B = flatten.backward(A)
output["flatten_backward"] = B[0]

class Model():
    def __init__(self):
        self.layers=[]

    def add(self, layer):
        self.layers.append(layer)

    def forward(self, X):
        A = X
        #print("A = ", A.shape)
        # GRADED FUNCTION: model
        ### START CODE HERE ### (≈ 5 lines)
        for l in range(len(self.layers)):
            if(self.layers[l].name=="flatten"):
                A=self.layers[l].forward(A).T # Transpose after flatten layer
                #print("if = ", A.shape)
            else:
                A=self.layers[l].forward(A)
                #print("else = ", A.shape)
        ### END CODE HERE ###
        return A

    def backward(self, AL=None, Y=None):
        L = len(self.layers)

        # GRADED FUNCTION: model
        ### START CODE HERE ### (≈ 7 lines)
        if self.layers[-1].name == "sigmoid":
            dAL = dAL = -1 * (np.divide(Y, AL + 1e-5) - np.divide(1 - Y, 1 - AL + 1e-5))
            dZ = self.layers[-1].backward(dA=dAL)  #activation layer backward
            dA_prev = self.layers[-2].backward(dZ) #linear layer backward
        else:
            dZ = self.layers[-1].backward(Y=Y)
            dA_prev = self.layers[-2].backward(dZ)
        ### END CODE HERE ###


        # Loop from l=L-3 to l=0
        # GRADED FUNCTION: model
        ### START CODE HERE ### (≈ 5 lines)
        for l in reversed(range(L-2)):
            if(self.layers[l].name=="flatten"):
                dA_prev=self.layers[l].backward(dA_prev.T) # Transpose before goes into flatten layer
            else:
                dA_prev=dA_prev=self.layers[l].backward(dA_prev)
        ### END CODE HERE ###

        return dA_prev

    def update(self, learning_rate):
        """
        Arguments:
        learning_rate -- step size
        """

        # GRADED FUNCTION: model
        # Only convolution layer and dense layer have to update parameters
        ### START CODE HERE ### (≈ 3 lines)
        L = len(self.layers)
        for i in range(L):
          if(self.layers[i].name=="dense" or self.layers[i].name=="conv"):
            self.layers[i].update(learning_rate)
        ### END CODE HERE ###

np.random.seed(seed)
A = np.random.randn(4,10,10,3)
Y = np.array([[1,0,1,0]])

model=Model()
model.add(Conv(filter_size=3, input_channel=3, output_channel=8, pad=1, stride=2))
model.add(Activation("relu"))
model.add(MaxPool(filter_size=2, stride=2))
model.add(Flatten())
model.add(Dense(32, 1))
model.add(Activation("sigmoid"))


AL = model.forward(A)
dA_prev = model.backward(AL=AL, Y=Y)
model.update(0.01)

print(model.layers[0].dW[0,0,0])
print(model.layers[0].db)
print(model.layers[4].dW[0,:8])
print(model.layers[4].db)


np.random.seed(seed)
A = np.random.randn(4,8,8,3)
Y = np.array([[1,1,0,0]])

model=Model()
model.add(Conv(filter_size=3, input_channel=3, output_channel=16, pad=1, stride=2))
model.add(Activation("relu"))
model.add(MaxPool(filter_size=2, stride=2))
model.add(Flatten())
model.add(Dense(64, 1))
model.add(Activation("sigmoid"))


AL = model.forward(A)
dA_prev = model.backward(AL=AL, Y=Y)
model.update(0.001)

output["model_1"] = model.layers[0].dW[0,0,0]
output["model_2"] = model.layers[0].db
output["model_3"] = model.layers[4].dW[0,:8]
output["model_4"] = model.layers[4].db

def compute_CCE_cost(AL, Y):
    """
    Implement the categorical cross-entropy cost function using the above formula.

    Arguments:
    AL -- probability vector corresponding to your label predictions, shape (number of classes, number of examples)
    Y -- true "label" vector (one hot vector, for example: [[1], [0], [0]] represents rock, [[0], [1], [0]] represents paper, [[0], [0], [1]] represents scissors
                              in a Rock-Paper-Scissors image classification), shape (number of classes, number of examples)

    Returns:
    cost -- categorical cross-entropy cost
    """

    m = Y.shape[1]
    #print(Y.shape)
    # Compute loss from aL and y.
    ### START CODE HERE ### (≈ 1 line of code)
    #cost = (-1/m)*np.dot(np.dot(Y, np.log(AL.T+1e-5)))
    cost = np.array((-1/m)*np.sum(np.multiply(Y,np.log(AL + 1e-5))))
    ### END CODE HERE ###

    cost = np.squeeze(cost)      # To make sure your cost's shape is what we expect (e.g. this turns [[17]] into 17).
    assert(cost.shape == ())

    return cost

PATH = "/content/drive/MyDrive/Colab Notebooks/Training_data"  #path to your training image
file_dir = os.listdir(PATH) #read the images from the directory
file_dir.sort() #Make sure the images are loaded in order
X_train = np.array([])

# Prepare X_train
# The shape of X_train will be (number of examples, height of image, width of image, channel of image)
# GRADED CODE: Binary classification (Data preprocessing)
# hint: use imread(PATH, IMREAD_GRAYSCALE) to load image
### START CODE HERE ### (≈ 9 line)
#cv2.imread("path", cv2.IMREAD_GRAYSCALE).
#print(file_dir)
X_in = []
for image in file_dir:  #iterate through each file to perform some action
    img = imread("/content/drive/MyDrive/Colab Notebooks/Training_data/{image}".format(image=image),IMREAD_GRAYSCALE)
    #X_in.append(img.reshape(img.shape[0], -1))
    #X_in.append(img.reshape(img.shape))
    X_in.append(img)

#df_train = read_csv("/content/drive/MyDrive/Colab Notebooks/Training_label.csv", index_col=0)
"""
for i in range(9):
	## define subplot
	plt.subplot(330 + 1 + i)
	## plot raw pixel data
	plt.imshow(X_in[i], cmap='gray', vmin=0, vmax=255)
## show the figure
plt.show()
"""
#X_train = np.array(X_in)
X_train = np.expand_dims(X_in,-1)
norm = X_train
X_train = (norm - norm.min()) / (norm.max() - norm.min())
#X_train = X_train/255.
#X_train = X_train.T

print(X_train.shape)
#print(X_train)
#print(X_train)
#original_images = [f"./content/drive/MyDrive/Colab Notebooks/Training_data" for file_name in PATH]
#original_images = [imread(file) for file in original_images]

### END CODE HERE ###

PATH = "/content/drive/MyDrive/Colab Notebooks/Testing_data"  #path to your testing image
file_dir = os.listdir(PATH)
file_dir.sort()
X_test = np.array([])

# Prepare X_test
# The shape of X_teset will be (number of examples, height of image, width of image, channel of image)
# GRADED CODE: Binary classification (Data preprocessing)
### START CODE HERE ### (≈ 9 line)
X_test_in = []
for images in file_dir:  #iterate through each file to perform some action
    #print(images)
    imge = imread("/content/drive/MyDrive/Colab Notebooks/Testing_data/{images}".format(images=images),IMREAD_GRAYSCALE)
    X_test_in.append(imge)
    #X_in.append(image.shape)
#X_test_in = np.array(X_test_in)
#print(X_test_in.shape)
"""
for i in range(9):
	## define subplot
	plt.subplot(330 + 1 + i)
	## plot raw pixel data
	plt.imshow(X_test_in[i], cmap='gray', vmin=0, vmax=255)
## show the figure
plt.show()
"""
X_test = np.expand_dims(X_test_in,-1)
norm = X_test
X_test = (norm - norm.min()) / (norm.max() - norm.min())
#X_test = X_test/255.
#X_test = X_test.T
print(X_test.shape)
#print(X_test)
### END CODE HERE ###

data = read_csv("Training_label.csv")

y_train = []
# Prepare y_train
# The shape of y_train will be (number of examples, 1), we will transpose y_train latter.
# GRADED CODE: Binary classification (Data preprocessing)
### START CODE HERE ### (≈ 2 line)
y_train = np.expand_dims(data.iloc[:,-1],-1)
print(y_train.shape)
#print(y_train)
### END CODE HERE ###

from sklearn.model_selection import train_test_split
#You can split training and validation set here. (Optional)
### START CODE HERE ###

### END CODE HERE ###

y_train = y_train.T #transpose y_train

# plot first few images
for i in range(9):
    # define subplot
    plt.subplot(330 + 1 + i)
    # plot raw pixel data
    plt.imshow(X_train[i].squeeze(), cmap='gray', vmin=0, vmax=1)
# show the figure
plt.show()

# check the shape of training data and testing data
print('Train: X=%s, y=%s' % (X_train.shape, y_train.shape))
print('Test: X=%s' % (X_test.shape, ))

def random_mini_batches(X, Y, mini_batch_size = 64):
    """
    Creates a list of random minibatches from (X, Y)

    Arguments:
    X -- input data, of shape !!!!!!!!!!!(number of examples ,input size)!!!!!!!!!!!
    Y -- true "label" vector, of shape (number of classes, number of examples)
    mini_batch_size -- size of the mini-batches, integer

    Returns:
    mini_batches -- list of synchronous (mini_batch_X, mini_batch_Y)
    """

    m = X.shape[0]  # number of training examples
    mini_batches = []

    # GRADED CODE: Binary classification
    ### START CODE HERE ###

    # Step 1: Shuffle (X, Y)
    permutation = list(np.random.permutation(m))
    shuffled_X = X[permutation,:]
    shuffled_Y = Y[:,permutation]

    inc = mini_batch_size

    # Step 2 - Partition (shuffled_X, shuffled_Y).
    # Cases with a complete mini batch size only i.e each of 64 examples.
    num_complete_minibatches = math.floor(m / mini_batch_size) # number of mini batches of size mini_batch_size in your partitionning
    for k in range(0, num_complete_minibatches):
        # (approx. 2 lines)
        mini_batch_X = shuffled_X[k * mini_batch_size : k * mini_batch_size + mini_batch_size, :]
        mini_batch_Y = shuffled_Y[:, k * mini_batch_size: k * mini_batch_size + mini_batch_size]
        mini_batch = (mini_batch_X, mini_batch_Y)
        mini_batches.append(mini_batch)

    # For handling the end case (last mini-batch < mini_batch_size i.e less than 64)
    if m % mini_batch_size != 0:
        #(approx. 2 lines)
        mini_batch_X = shuffled_X[num_complete_minibatches * mini_batch_size :, :]
        mini_batch_Y = shuffled_Y[:, num_complete_minibatches * mini_batch_size : ]
        mini_batch = (mini_batch_X, mini_batch_Y)
        mini_batches.append(mini_batch)


    return mini_batches

    ### END CODE HERE ###

# GRADED CODE: Binary classification
### START CODE HERE ###
learning_rate = 0.0036
num_iterations = 22
batch_size = 64
print_cost = True
classes = 2
costs = []   # keep track of cost


# build the model
model=Model()
model.add(Conv(filter_size=3, input_channel=1, output_channel=16, pad=0, stride=2))
model.add(Activation("relu"))
model.add(MaxPool(filter_size=2, stride=2))
model.add(Conv(filter_size=3, input_channel=16, output_channel=16, pad=0, stride=2))
model.add(Activation("relu"))

model.add(Flatten())
model.add(Dense(144, 32))
model.add(Activation("relu"))
model.add(Dense(32, 1))
model.add(Activation("sigmoid"))

# Loop (gradient descent)
for i in range(0, num_iterations):
    print("epoch: ",i)
    mini_batches = random_mini_batches(X_train, y_train, batch_size)
    j=0
    for batch in mini_batches:
        x_batch, y_batch = batch

        # forward
        AL = model.forward(x_batch)

        # compute cost
        if classes == 2:
            cost = compute_BCE_cost(np.array(AL),np.array(y_batch))
        else:
            cost = compute_CCE_cost(np.array(AL) ,np.array(y_batch))

        # backward
        dA_prev = model.backward(AL, y_batch)

        # update
        model.update(learning_rate)

    print ("Cost after iteration %i: %f" %(i, cost))
    costs.append(cost)
### END CODE HERE ###

# plot the cost
plt.plot(np.squeeze(costs))
plt.ylabel('cost')
plt.xlabel('iterations (per hundreds)')
plt.title("Learning rate =" + str(learning_rate))
plt.show()

plt.plot(np.squeeze(costs))
plt.ylabel('Loss')
plt.xlabel('Epoch')
plt.show()

pred_train = predict(X_train, y_train, model, 2)


pred_test = predict(X_test, None, model, 2)
print(pred_test)
output["basic_pred_test"] = pred_test[0].astype(int)

basic_model_layers = []
basic_model_parameters = []
for layer in model.layers:
    basic_model_layers.append(layer.name)
    if(layer.name=="conv" or layer.name=="dense" or layer.name=="maxpool"):
        basic_model_parameters.append(layer.parameters)
output["basic_model_layers"] = basic_model_layers
output["basic_model_parameters"] = basic_model_parameters

# import some tensorflow packages to help you build the model
import tensorflow as tf
from tensorflow.keras import layers, models

# import other packages here
from keras.models import Sequential
from keras.layers import Conv2D, MaxPooling2D, BatchNormalization
from keras.layers import Activation, Dropout, Flatten, Dense
import pandas as pd

# GRADED CODE: Advanced implementation
### Data preprocess & augmentation ###
PATH = "/content/drive/MyDrive/Colab Notebooks/Training_data"  #path to your training image
file_dir = os.listdir(PATH) #read the images from the directory
file_dir.sort() #Make sure the images are loaded in order
adv_X_train = np.array([])

adv_X_in = []
for image in file_dir:  #iterate through each file to perform some action
    img = imread("/content/drive/MyDrive/Colab Notebooks/Training_data/{image}".format(image=image),IMREAD_GRAYSCALE)
    adv_X_in.append(img)
#X_in = np.expand_dims(X_in,-1)
adv_X_train = np.expand_dims(adv_X_in,-1)
#adv_X_train = np.array(adv_X_in)
print(adv_X_train.shape)

PATH_TEST = "/content/drive/MyDrive/Colab Notebooks/Testing_data"  #path to your testing image
file_dir_test = os.listdir(PATH_TEST)
file_dir_test.sort()
X_test = np.array([])

X_test_in = []
for images in file_dir_test:  #iterate through each file to perform some action
    #print(images)
    imge = imread("/content/drive/MyDrive/Colab Notebooks/Testing_data/{images}".format(images=images),IMREAD_GRAYSCALE)
    X_test_in.append(imge)

X_test = np.expand_dims(X_test_in,-1)
#X_test = np.array(X_test_in)
print(X_test.shape)

data = read_csv("Training_label.csv")

y_train_in = []
y_train_in = data.iloc[:,-1]
#print(y_train_in)
y_train_in = np.ravel(y_train_in)
#y_train_in = y_train_in.flat()
print(y_train_in.shape)
m = 600

permutation = list(np.random.permutation(m))
shuffled_X = adv_X_train[permutation,:]
shuffled_Y = y_train_in[permutation]

x_train_adv, y_train_adv = shuffled_X[:400, :], shuffled_Y[:400]
X_val, y_val = shuffled_X[400:, :], shuffled_Y[400:]
#print(x_train_adv)
xTrainAdv = tf.keras.utils.normalize(x_train_adv, axis=1)
#print(xTrainAdv)
xVal = tf.keras.utils.normalize(X_val, axis=1)

for i in range(9):
    # define subplot
    plt.subplot(330 + 1 + i)
    # plot raw pixel data
    plt.imshow(xTrainAdv[i].squeeze(), cmap='gray', vmin=0, vmax=1)
# show the figure
plt.show()

print("shape of X_train: " + str(xTrainAdv.shape) + " shape of y_train: " + str(y_train_adv.shape))
print("shape of X_val: " + str(xVal.shape) + " shape of y_val: " + str(y_val.shape))

tf.keras.utils.set_random_seed(1)

model = tf.keras.Sequential([

tf.keras.layers.Conv2D(32, (3, 3), activation='relu', padding='same', input_shape=(32, 32, 1)),
tf.keras.layers.MaxPooling2D((2, 2), strides=2),

tf.keras.layers.Conv2D(64, (3, 3), activation='relu', padding='same'),
tf.keras.layers.MaxPooling2D((2, 2), strides=2),

tf.keras.layers.Conv2D(128, (3, 3), activation='relu', padding='same'),
tf.keras.layers.MaxPooling2D((2, 2), strides=2),

tf.keras.layers.Conv2D(256, (3, 3), activation='relu', padding='same'),
tf.keras.layers.MaxPooling2D((2, 2), strides=2),

tf.keras.layers.Conv2D(512, (3, 3), activation='relu', padding='same'),
tf.keras.layers.MaxPooling2D((2, 2), strides=2),

tf.keras.layers.Flatten(),
tf.keras.layers.Dense(32, activation='relu'),
tf.keras.layers.Dense(1, activation='sigmoid'),
])

model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])
print(model.summary())

historyAdv = model.fit(np.array(xTrainAdv), np.array(y_train_adv), epochs=22, validation_data=(np.array(xVal), np.array(y_val)))

# GRADED CODE: Advanced implementation
### Start training ###
#metrics_df = pd.DataFrame(historyAdv.history)
#metrics_df["loss"].plot();
plt.plot((historyAdv.history["loss"]))
plt.ylabel('Loss')
plt.xlabel('Epoch')
plt.show()

#pred_test1 = model.predict(xTrainAdv)
#pred_test2 = model.predict(xVal)
pred_test = model.predict(X_test)
#pred_test = np.argmax(pred_test, axis = 1)
pred_test = 1 * ((pred_test)>0.5)
output["advanced_pred_test"] = pred_test
print(pred_test.flatten())
print(type(pred_test))

# sanity check
assert(list(output.keys()) == ['conv_initialization', 'zero_padding', 'conv_single_step', 'conv_forward_1', 'conv_forward_2', 'conv_forward_3', 'conv_update_1', 'conv_update_2', 'maxpool_forward', 'flatten_forward', 'flatten_backward', 'model_1', 'model_2', 'model_3', 'model_4', 'basic_pred_test', 'basic_model_layers', 'basic_model_parameters', 'advanced_pred_test'])

np.save("hw4_output.npy", output)

# sanity check
submit = np.load("hw4_output.npy", allow_pickle=True).item()
for key, value in submit.items():
    print(str(key) + "： " + str(type(value)))